{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "# # 导入PyTorch库\n",
    "# import torch\n",
    "\n",
    "# # 生成标签为张量类型的字典\n",
    "# labels_tensor_dict = {i: torch.tensor([0]) if i <= 34 else torch.tensor([1]) for i in range(1, 69)}\n",
    "# print(labels_tensor_dict)\n",
    "# 生成字典，其中1-34的标签为0，35-68的标签为1\n",
    "observer_grouth = {i: 0 if i <= 34 else 1 for i in range(1, 69)}\n",
    "observer_grouth\n",
    "with open('/home/pliu/code/copy_modify_beit/test', 'a') as file:\n",
    "    for key in observer_grouth:\n",
    "        print(key)\n",
    "        file.write(f\"Observer {key}: Correct\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy for all observers is: 95.59, found at line 1242\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 假设你的文本文档路径为 'path/to/your/file.txt'\n",
    "file_path = '/home/pliu/code/copy_modify_beit/save_result_7/evaluation_results.txt'\n",
    "\n",
    "# 初始化最大准确率值及其行号\n",
    "max_accuracy = None\n",
    "max_accuracy_line = None\n",
    "\n",
    "# 打开并读取文件\n",
    "with open(file_path, 'r') as file:\n",
    "    for line_number, line in enumerate(file, start=1):\n",
    "        # 使用正则表达式匹配 \"Accuracy for all observers:\" 后的数字\n",
    "        match = re.search(r'Accuracy for all observers:\\s*([0-9.]+)', line)\n",
    "        if match:\n",
    "            # 将匹配的数字（字符串形式）转换为浮点数\n",
    "            accuracy_value = float(match.group(1))\n",
    "            # 如果找到的准确率值大于当前最大值，更新最大值及其行号\n",
    "            if max_accuracy is None or accuracy_value > max_accuracy:\n",
    "                max_accuracy = accuracy_value\n",
    "                max_accuracy_line = line_number\n",
    "\n",
    "# 检查是否找到了最大的准确率值及其行号\n",
    "if max_accuracy is not None:\n",
    "    print(f\"The maximum accuracy for all observers is: {max_accuracy}, found at line {max_accuracy_line}\")\n",
    "else:\n",
    "    print(\"No accuracy values found in the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 0, 3: 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设已有字典 observer_outputs 包含每个受试者的两个概率值，例如：\n",
    "observer_outputs = {\n",
    "    1: [0.2, 0.8],\n",
    "    2: [0.7, 0.3],\n",
    "    3: [0.4, 0.6],\n",
    "    # 更多受试者\n",
    "}\n",
    "\n",
    "# 计算每个受试者最大概率的索引，并将结果存储在新字典中\n",
    "observer_max_prob_index = {observer: torch.tensor(probs).argmax().item() for observer, probs in observer_outputs.items()}\n",
    "\n",
    "observer_max_prob_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m extracted_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      2\u001b[0m randn_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m randn_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandn_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      5\u001b[0m t \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m1\u001b[39m:, :]\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "extracted_features = torch.zeros(2, 3, 2)\n",
    "randn_tensor = torch.randn(9, 1)\n",
    "randn_tensor = torch.stack(randn_tensor).long()\n",
    "x = torch.randn(1, 3, 2)\n",
    "t = x[:, 1:, :]\n",
    "print(x)\n",
    "print(t)\n",
    "print(randn_tensor)\n",
    "print(randn_tensor[1, :])\n",
    "extracted_features[:, 0, :] = t[torch.arange(2), randn_tensor[0, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[169]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_patch_indices(position_fixations, img_size, patch_size):\n",
    "    # position_fixations是注视点坐标的列表，每个元素格式为(x, y)\n",
    "    # img_size是图像尺寸，例如(224, 224)\n",
    "    # patch_size是块的大小，例如16\n",
    "    if position_fixations is None or position_fixations is []:\n",
    "# 如果position_fixations为空，则返回空列表或适当的默认值\n",
    "        return None\n",
    "    patch_indices = []\n",
    "    for (x, y) in position_fixations:\n",
    "        i = y // patch_size\n",
    "        # j = x // patch_size + 1\n",
    "        if x != 224:\n",
    "            j = x // patch_size + 1\n",
    "        else:\n",
    "            j = x // patch_size\n",
    "        if i != 14:\n",
    "            patch_idx = i * (img_size // patch_size) + j \n",
    "        else:\n",
    "            patch_idx = (i - 1) * (img_size // patch_size) + j \n",
    "        patch_indices.append(patch_idx)\n",
    "    return patch_indices\n",
    "\n",
    "\n",
    "get_patch_indices([(224, 191)], 224, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2427, 0.4194, 0.3373],\n",
      "         [0.0564, 0.2438, 0.0110]],\n",
      "\n",
      "        [[0.9959, 0.7933, 0.7369],\n",
      "         [0.8466, 0.4912, 0.2159]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pliu/anaconda3/envs/beit/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2427, 0.0564],\n",
       "        [0.9959, 0.8466]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "img=torch.rand(2,2,3)\n",
    "print(img)\n",
    "img[:,:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Commas replaced with spaces in all text files.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory path where the text files are located\n",
    "# Assuming the uploaded file is in the same directory as the other text files\n",
    "directory_path = '/home/pliu/code/train_modify_beit/Saliency4ASD/TD/'\n",
    "\n",
    "# Define a function to replace commas with spaces in a single file\n",
    "def replace_commas_with_spaces(file_path):\n",
    "    # Read the content of the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    # Replace commas with spaces\n",
    "    new_content = content.replace(',', ' ')\n",
    "    \n",
    "    # Write the new content back to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(new_content)\n",
    "\n",
    "# Get a list of all text files in the directory\n",
    "text_files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]\n",
    "\n",
    "# Apply the replacement to each file\n",
    "for text_file in text_files:\n",
    "    file_path = os.path.join(directory_path, text_file)\n",
    "    replace_commas_with_spaces(file_path)\n",
    "\n",
    "# Confirm the task is done\n",
    "\"Commas replaced with spaces in all text files.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Columns added to the first 300 text files in the directory.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the user wants to add a column with incremental numbers to the text files in a folder,\n",
    "# we will write a Python function to process the file that was uploaded as an example,\n",
    "# and then explain how this can be applied to multiple files in a folder.\n",
    "\n",
    "# First, we need to read the example file, process it, and save it with the new column.\n",
    "def add_column_to_text(file_path, number):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Adding a number at the end of each line\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for line in lines:\n",
    "            if line.strip():  # ignore empty lines\n",
    "                file.write(f\"{line.strip()} {number}\\n\")\n",
    "            else:\n",
    "                file.write(\"\\n\")  # keep empty lines\n",
    "\n",
    "# The path to the directory where the files are stored\n",
    "directory = '/home/pliu/code/train_modify_beit/Saliency4ASD/ASD/'\n",
    "\n",
    "# List all files in the directory\n",
    "files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "\n",
    "# We will sort the files to ensure they are in order\n",
    "files.sort()\n",
    "\n",
    "# We will process only the first 300 text files due to the user's specification\n",
    "for i, filename in enumerate(files[:300]):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    add_column_to_text(file_path, i+1)  # We add 1 because enumeration starts at 0\n",
    "\n",
    "# Confirming the task completion\n",
    "\"Columns added to the first 300 text files in the directory.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_files(folder_path):\n",
    "    \n",
    "    new_files_data = {}  # 用于存储新文件的数据，键为文件编号，值为数据列表\n",
    "    # files_data = {}\n",
    "\n",
    "\n",
    "    for i in range(1,301):  # 限制处理前300个文件\n",
    "        file_count = 0  # 用于记录新创建的文本文件数量\n",
    "        file_path = os.path.join(folder_path, f\"{i}.txt\")\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                columns = line.strip().split()\n",
    "                if columns[0] == '0':  # 第一列为0\n",
    "                    file_count += 1  # 更新文件编号\n",
    "                    new_files_data[file_count] = []  # 初始化新文件的数据存储\n",
    "                # 将当前行添加到对应编号的文件数据中\n",
    "                new_files_data[file_count].append(line.strip())\n",
    "                \n",
    "            # 将累积的数据写入新文件\n",
    "            for file_num, lines in new_files_data.items():\n",
    "                new_file_path = os.path.join('./backup/sub/TD/', f'{file_num}.txt')\n",
    "                with open(new_file_path, 'a') as new_file:\n",
    "                    new_file.write('\\n'.join(lines) + '\\n')\n",
    "\n",
    "process_files('./backup/TD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件更名完成。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 文件夹路径，如果脚本和文件在同一目录下，可以设置为 \".\"\n",
    "folder_path = \"./dataset1/dataset_ASD_1/val/TD/data\"\n",
    "\n",
    "for i in range(35, 69):  # 从1到30\n",
    "    old_name = os.path.join(folder_path, f\"{i}.txt\")\n",
    "    new_name = os.path.join(folder_path, f\"{i+58}.txt\")\n",
    "    os.rename(old_name, new_name)  # 更改文件名\n",
    "\n",
    "print(\"文件更名完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def copy_folder(source_folder, destination_folder, num_copies):\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "    \n",
    "    # Copy the source folder to the destination folder multiple times\n",
    "    for i in range(num_copies):\n",
    "        shutil.copytree(source_folder, os.path.join(destination_folder, f\"group{i+1}\"))\n",
    "\n",
    "# Example usage:\n",
    "source_folder = \"./dataset_task_cross5/group\"\n",
    "destination_folder = \"./dataset_task_cross5\"\n",
    "num_copies = 5\n",
    "\n",
    "copy_folder(source_folder, destination_folder, num_copies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 3, 5, 6, 10, 2, 13]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def getRandomIndex(n, x):\n",
    "\t# 索引范围为[0, n), 随机选x个不重复\n",
    "    index = random.sample(range(n), x)\n",
    "    return index\n",
    "index = getRandomIndex(14, 7)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239\n"
     ]
    }
   ],
   "source": [
    "x = 239\n",
    "if 16 <= x <= 240:\n",
    "    print(x)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理视觉任务数据集\n",
    "# 假设原文件路径\n",
    "original_file_path = 'ExplicitJudgments.txt'\n",
    "\n",
    "# 初始化一个字典来存储不同obs值对应的数据\n",
    "data_by_obs = {}\n",
    "\n",
    "with open(original_file_path, 'r') as file:\n",
    "    # 跳过标题行\n",
    "    next(file)\n",
    "    # 读取每一行\n",
    "    for line in file:\n",
    "        # 分割行以获取数据\n",
    "        # x, y, obs, image, fixation = line.strip().split()\n",
    "        x, y, obs, image = line.strip().split()\n",
    "        \n",
    "        # 检查x或y是否为'NaN'或空字符串（根据您的数据格式调整）\n",
    "        if x == 'NaN' or y == 'NaN' or x == '' or y == '':\n",
    "            continue  # 如果x或y是NaN，则跳过这行数据\n",
    "        \n",
    "        # 检查obs值是否已存在于字典中\n",
    "        if obs not in data_by_obs:\n",
    "            data_by_obs[obs] = []\n",
    "        \n",
    "        # 修正fixation的序号为从0开始\n",
    "        # fixation_num = int(fixation) - 1  # 假设原始数据中fixation从1开始\n",
    "        \n",
    "        # 准备要写入的数据格式\n",
    "        formatted_line = f\"{0} {x} {y} 'image_r_{image}'\\n\"\n",
    "        # formatted_line = f\"{fixation_num} {x} {y} 'image_r_{image}'\\n\"\n",
    "        \n",
    "        # 将格式化后的数据行添加到相应的obs列表中\n",
    "        data_by_obs[obs].append(formatted_line)\n",
    "\n",
    "# 遍历data_by_obs字典并将数据写入到相应的文件中\n",
    "for obs, lines in data_by_obs.items():\n",
    "    # 创建或覆盖文本文件\n",
    "    with open(f\"./dataset_task/ExplicitJudgments/{obs}.txt\", 'w') as file:\n",
    "        file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(3, 4)\n",
    "concatenated_tensor = torch.cat((tensor1, tensor2), dim=1)\n",
    "print(concatenated_tensor.shape)\n",
    "stacked_tensor = torch.stack((tensor1, tensor2), dim=0)\n",
    "print(stacked_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def update_matrix(matrix):\n",
    "    rows = len(matrix)\n",
    "    cols = len(matrix[0])\n",
    "\n",
    "    # 创建一个与原始矩阵相同大小的新矩阵，用于存储更新后的结果\n",
    "    new_matrix = [[0 for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "    # 定义八个方向的偏移量\n",
    "    directions = [(i, j) for i in [-1, 0, 1] for j in [-1, 0, 1] if (i, j) != (0, 0)]\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if matrix[i][j] == 1:\n",
    "                new_matrix[i][j] = 1  # 将当前位置设置为1\n",
    "\n",
    "                # 更新周围八个位置\n",
    "                for dx, dy in directions:\n",
    "                    x, y = i + dx, j + dy\n",
    "                    if 0 <= x < rows and 0 <= y < cols:\n",
    "                        new_matrix[x][y] = 1\n",
    "\n",
    "    return new_matrix\n",
    "\n",
    "# 测试示例\n",
    "matrix = [\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "updated_matrix = update_matrix(matrix)\n",
    "for row in updated_matrix:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_curve, auc\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_curve, auc\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/beit/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/beit/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "y_true=np.array([0,0,1,1])\n",
    "y_score=np.array([0.1,0.4,0.35,0.8])   # 概率值选择的是正类的概率\n",
    "fpr,tpr,threshold=roc_curve(y_true,y_score)\n",
    "roc_auc = auc(fpr,tpr)   # 准确率代表所有正确的占所有数据的比值\n",
    "print('roc_auc:', roc_auc)\n",
    "lw = 2\n",
    "plt.subplot(1,1,1)\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)  # 假正率为横坐标，真正率为纵坐标做曲线\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC', y=0)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设的sub_outputs数据（通常这里是模型输出的原始分数，称为logits）\n",
    "sub_outputs = {\n",
    "    6: [0.07696998949432373, -0.065694211931531318],\n",
    "    1: [0.06857764720916748, -0.066603799438475656]\n",
    "}\n",
    "\n",
    "# 应用softmax函数转换logits为概率\n",
    "sub_probabilities = {subj: F.softmax(torch.tensor(logits), dim=0).tolist() for subj, logits in sub_outputs.items()}\n",
    "\n",
    "# 假设的sub_label数据\n",
    "sub_label = {6: 0, 1: 1}\n",
    "\n",
    "# 准备写入文件\n",
    "save_path = '/mnt/data/probabilities_and_labels.txt'\n",
    "\n",
    "# 写入文件\n",
    "with open(save_path, 'w') as f:\n",
    "    for subj in sub_outputs:\n",
    "        probabilities = sub_probabilities[subj]\n",
    "        label = sub_label[subj]\n",
    "        f.write(f'{subj}: {probabilities}, label: {label}\\n')\n",
    "        \n",
    "# 检查输出文件的内容\n",
    "with open(save_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
